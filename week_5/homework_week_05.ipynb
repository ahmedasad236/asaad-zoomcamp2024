{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/04 22:46:17 WARN Utils: Your hostname, asaad resolves to a loopback address: 127.0.1.1; using 192.168.1.26 instead (on interface wlp61s0)\n",
      "24/03/04 22:46:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/04 22:46:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/03/04 22:46:29 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/03/04 22:46:29 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "24/03/04 22:46:29 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = pd.read_csv('fhv_tripdata_2019-10.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dispatching_base_num       object\n",
       "pickup_datetime            object\n",
       "dropOff_datetime           object\n",
       "PUlocationID              float64\n",
       "DOlocationID              float64\n",
       "SR_Flag                   float64\n",
       "Affiliated_base_number     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    df_pandas.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema with correct column names\n",
    "schema = types.StructType([\n",
    "    types.StructField('Affiliated_base_number', types.StringType(), True),\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropOff_datetime', types.TimestampType(), True),  # Corrected column name\n",
    "    types.StructField('PUlocationID', types.IntegerType(), True),  # Corrected column name\n",
    "    types.StructField('DOlocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.DoubleType(), True)  # Assuming SR_Flag is a numeric field\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('fhv_tripdata_2019-10.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/04 23:48:01 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: dispatching_base_num, pickup_datetime, dropOff_datetime, PUlocationID, DOlocationID, SR_Flag, Affiliated_base_number\n",
      " Schema: Affiliated_base_number, dispatching_base_num, pickup_datetime, dropOff_datetime, PUlocationID, DOlocationID, SR_Flag\n",
      "Expected: Affiliated_base_number but found: dispatching_base_num\n",
      "CSV file: file:///home/asaasd/spark/homework/fhv_tripdata_2019-10.csv.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 38:>                                                         (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "df.write.parquet('fhvhv/2019/10/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('fhvhv/2019/10/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+--------------------+-------------------+----------------+------------+------------+-------+\n",
      "|Affiliated_base_number|dispatching_base_num|    pickup_datetime|dropOff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+----------------------+--------------------+-------------------+----------------+------------+------------+-------+\n",
      "|                B00837| 2019-10-03 09:48:29|2019-10-03 11:21:47|            null|         264|        null| B00837|\n",
      "|                B00009| 2019-10-30 09:26:00|2019-10-30 09:43:00|            null|         264|        null| B00009|\n",
      "|                B02594| 2019-10-04 19:27:00|2019-10-04 20:06:00|            null|         132|        null| B02594|\n",
      "|                B00647| 2019-10-24 13:25:09|2019-10-24 15:40:36|            null|         241|        null| B00647|\n",
      "|                B00900| 2019-10-14 06:17:14|2019-10-14 06:21:16|            null|         197|        null| B00900|\n",
      "|                B02719| 2019-10-10 07:10:40|2019-10-10 07:25:31|            null|         213|        null| B02719|\n",
      "|                B01509| 2019-10-14 15:25:56|2019-10-14 15:30:32|            null|         149|        null| B01509|\n",
      "|                B03016| 2019-10-11 23:28:01|2019-10-11 23:59:59|            null|         265|        null| B02883|\n",
      "|                B00900| 2019-10-30 11:39:16|2019-10-30 11:55:16|            null|         196|        null| B00900|\n",
      "|                B02546| 2019-10-12 19:11:56|2019-10-12 19:22:34|            null|         167|        null| B02546|\n",
      "|                B01231| 2019-10-10 13:02:10|2019-10-10 13:51:13|            null|          75|        null| B02883|\n",
      "|                B01436| 2019-10-09 06:04:03|2019-10-09 06:20:38|            null|         112|        null| B02869|\n",
      "|                B00319| 2019-10-26 15:30:12|2019-10-26 15:36:00|            null|         264|        null| B00319|\n",
      "|                B01250| 2019-10-23 08:30:00|2019-10-23 09:10:00|            null|         264|        null| B01250|\n",
      "|                B01061| 2019-10-03 04:44:12|2019-10-03 04:47:17|            null|         250|        null| B01061|\n",
      "|                B00310| 2019-10-26 13:58:15|2019-10-26 14:00:07|            null|         212|        null| B00310|\n",
      "|                B01239| 2019-10-16 09:40:21|2019-10-16 09:48:30|            null|          32|        null| B01239|\n",
      "|                B02437| 2019-10-16 19:31:48|2019-10-16 19:57:58|            null|         169|        null| B02437|\n",
      "|                B01509| 2019-10-22 10:06:52|2019-10-22 10:17:36|            null|         123|        null| B01509|\n",
      "|                B00254| 2019-10-08 08:56:12|2019-10-08 09:32:45|            null|          75|        null| B00254|\n",
      "+----------------------+--------------------+-------------------+----------------+------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+--------------------+---------------+----------------+------------+------------+-------+\n",
      "|Affiliated_base_number|dispatching_base_num|pickup_datetime|dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+----------------------+--------------------+---------------+----------------+------------+------------+-------+\n",
      "+----------------------+--------------------+---------------+----------------+------------+------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "not_null_rows = df.filter(col(\"DOLocationID\").isNotNull()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "october_15_trips = df.filter(df['dispatching_base_num'].cast(\"date\") == \"2019-10-15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count the number of trips\n",
    "number_of_trips = october_15_trips.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62610"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_trips"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
